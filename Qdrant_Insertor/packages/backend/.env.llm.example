# LLM服务配置示例
# 复制此文件为 .env 并根据需要修改配置

# LLM提供商 (openai, anthropic, azure, openai_compatible)
LLM_PROVIDER=openai

# LLM API密钥
LLM_API_KEY=your-llm-api-key-here

# LLM API基础URL (可选，默认使用提供商的默认URL)
LLM_BASE_URL=https://api.openai.com/v1

# LLM模型名称
LLM_MODEL=gpt-3.5-turbo

# LLM最大令牌数 (可选，默认4096)
LLM_MAX_TOKENS=4096

# LLM温度参数 (可选，默认0.1，范围0-2)
LLM_TEMPERATURE=0.1

# LLM请求超时时间，毫秒 (可选，默认30000)
LLM_TIMEOUT=30000

# Azure特定配置 (仅当LLM_PROVIDER=azure时需要)
LLM_API_VERSION=2024-02-15-preview
LLM_ORGANIZATION_ID=
LLM_PROJECT_ID=your-azure-project-id
LLM_DEPLOYMENT_NAME=your-azure-deployment-name

# 语义分块配置
# 是否启用LLM语义分块 (可选，默认false)
LLM_SEMANTIC_SPLITTING_ENABLED=true

# 目标块大小，字符数 (可选，默认1000)
LLM_TARGET_CHUNK_SIZE=1000

# 块重叠大小，字符数 (可选，默认100)
LLM_CHUNK_OVERLAP=100

# 最大块数量 (可选，无限制)
LLM_MAX_CHUNKS=10

# 分块策略 (coherent, topic-based, semantic, balanced，默认balanced)
LLM_SPLIT_STRATEGY=balanced

# 是否启用降级策略 (可选，默认true)
LLM_ENABLE_FALLBACK=true

# 降级策略 (by_size, by_headings, auto，默认auto)
LLM_FALLBACK_STRATEGY=auto

# 最大重试次数 (可选，默认3)
LLM_MAX_RETRIES=3

# 重试延迟，毫秒 (可选，默认1000)
LLM_RETRY_DELAY=1000

# 是否启用缓存 (可选，默认true)
LLM_ENABLE_CACHE=true

# 缓存TTL，毫秒 (可选，默认300000，即5分钟)
LLM_CACHE_TTL=300000

# 自定义请求头 (可选，JSON格式)
# LLM_HEADERS={"Custom-Header": "value"}

# OpenAI特定配置 (如果使用OpenAI作为LLM提供商)
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-3.5-turbo

# Anthropic特定配置 (如果使用Anthropic作为LLM提供商)
# ANTHROPIC_API_KEY=your-anthropic-api-key-here
# ANTHROPIC_BASE_URL=https://api.anthropic.com
# ANTHROPIC_MODEL=claude-3-sonnet-20240229